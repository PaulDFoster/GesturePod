{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:21:37.623679Z",
     "start_time": "2019-06-30T10:21:37.385149Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_edgeml.graph.protoNN import ProtoNN\n",
    "from pytorch_edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "import pytorch_edgeml.utils as utils\n",
    "import helpermethods as helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paul's Data\n",
    "It is assumed that the USPS data has already been downloaded and set up with the help of `fetch_usps.py` and is placed in the `./usps10` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:21:37.643781Z",
     "start_time": "2019-06-30T10:21:37.626020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39240, 124)\n",
      "(39240, 10)\n",
      "(39240, 124)\n",
      "(39240, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = './mpu'\n",
    "#train, test = np.load(DATA_DIR + '/train.npy'), np.load(DATA_DIR + '/test.npy')\n",
    "train = np.genfromtxt(DATA_DIR + '/_train.csv', delimiter=\",\", skip_header=1)\n",
    "test = np.genfromtxt(DATA_DIR + '/_test.csv', delimiter=\",\", skip_header=1)\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 2) # +2 to allow for zero index\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "# Load data\n",
    "train = np.genfromtxt(DATA_DIR + '/_train.csv', delimiter=\",\", skip_header=1)\n",
    "test = np.genfromtxt(DATA_DIR + '/_test.csv', delimiter=\",\", skip_header=1)\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "# Convert y to one-hot\n",
    "minval = 0\n",
    "#numClasses = int(10)\n",
    "y_train = helper.to_onehot(y_train, numClasses, minlabel=minval)\n",
    "y_test = helper.to_onehot(y_test, numClasses, minlabel=minval)\n",
    "dataDimension = x_train.shape[1]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable GAMMA. If GAMMA is set to None, median heuristic will be used to estimate a good value of $\\gamma$ through the helper.getGamma() method. This method also returns the corresponding W and B matrices which should be used to initialize ProtoNN (as is done here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:21:37.653094Z",
     "start_time": "2019-06-30T10:21:37.645811Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 10\n",
    "NUM_PROTOTYPES = 20\n",
    "REG_W = 0.0\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.0\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 1.0\n",
    "SPAR_Z = 1.0\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.0014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:21:37.680649Z",
     "start_time": "2019-06-30T10:21:37.654776Z"
    }
   },
   "outputs": [],
   "source": [
    "W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:21:37.700092Z",
     "start_time": "2019-06-30T10:21:37.682341Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse training disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using x-entropy loss\n"
     ]
    }
   ],
   "source": [
    "protoNNObj = ProtoNN(dataDimension, PROJECTION_DIM, NUM_PROTOTYPES, numClasses,\n",
    "                     gamma, W=W, B=B)\n",
    "protoNNTrainer = ProtoNNTrainer(protoNNObj, REG_W, REG_B, REG_Z, SPAR_W, SPAR_B, SPAR_W,\n",
    "                                LEARNING_RATE, lossType='xentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:25:13.443873Z",
     "start_time": "2019-06-30T10:21:37.702352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 batch 0 loss 2.302702 acc 0.000000\n",
      "Epoch 0 batch 600 loss 0.093539 acc 0.952381\n",
      "Epoch 1 batch 0 loss 0.070658 acc 0.968750\n",
      "Epoch 1 batch 600 loss 0.083276 acc 0.952381\n",
      "Epoch 2 batch 0 loss 0.062033 acc 0.968750\n",
      "Epoch 2 batch 600 loss 0.076922 acc 0.952381\n",
      "Epoch 3 batch 0 loss 0.059458 acc 0.968750\n",
      "Epoch 3 batch 600 loss 0.057150 acc 0.968254\n",
      "Epoch 4 batch 0 loss 0.054689 acc 0.968750\n",
      "Epoch 4 batch 600 loss 0.043011 acc 0.984127\n",
      "Epoch 5 batch 0 loss 0.041038 acc 0.984375\n",
      "Epoch 5 batch 600 loss 0.039837 acc 0.984127\n",
      "Epoch 6 batch 0 loss 0.037929 acc 0.984375\n",
      "Epoch 6 batch 600 loss 0.038347 acc 0.984127\n",
      "Epoch 7 batch 0 loss 0.036915 acc 0.984375\n",
      "Epoch 7 batch 600 loss 0.037573 acc 0.984127\n",
      "Epoch 8 batch 0 loss 0.036482 acc 0.984375\n",
      "Epoch 8 batch 600 loss 0.037232 acc 0.984127\n",
      "Epoch 9 batch 0 loss 0.036282 acc 0.984375\n",
      "Epoch 9 batch 600 loss 0.037039 acc 0.984127\n",
      "Validation accuracy: 0.969827\n",
      "Epoch 10 batch 0 loss 0.036191 acc 0.984375\n",
      "Epoch 10 batch 600 loss 0.036880 acc 0.984127\n",
      "Epoch 11 batch 0 loss 0.036140 acc 0.984375\n",
      "Epoch 11 batch 600 loss 0.036792 acc 0.984127\n",
      "Epoch 12 batch 0 loss 0.036095 acc 0.984375\n",
      "Epoch 12 batch 600 loss 0.036753 acc 0.984127\n",
      "Epoch 13 batch 0 loss 0.036071 acc 0.984375\n",
      "Epoch 13 batch 600 loss 0.036740 acc 0.984127\n",
      "Epoch 14 batch 0 loss 0.036060 acc 0.984375\n",
      "Epoch 14 batch 600 loss 0.036703 acc 0.984127\n",
      "Epoch 15 batch 0 loss 0.036044 acc 0.984375\n",
      "Epoch 15 batch 600 loss 0.036640 acc 0.984127\n",
      "Epoch 16 batch 0 loss 0.036026 acc 0.984375\n",
      "Epoch 16 batch 600 loss 0.036600 acc 0.984127\n",
      "Epoch 17 batch 0 loss 0.036033 acc 0.984375\n",
      "Epoch 17 batch 600 loss 0.036592 acc 0.984127\n",
      "Epoch 18 batch 0 loss 0.036066 acc 0.984375\n",
      "Epoch 18 batch 600 loss 0.036672 acc 0.984127\n",
      "Epoch 19 batch 0 loss 0.036034 acc 0.984375\n",
      "Epoch 19 batch 600 loss 0.036621 acc 0.984127\n",
      "Validation accuracy: 0.970540\n",
      "Epoch 20 batch 0 loss 0.036013 acc 0.984375\n",
      "Epoch 20 batch 600 loss 0.036572 acc 0.984127\n",
      "Epoch 21 batch 0 loss 0.035995 acc 0.984375\n",
      "Epoch 21 batch 600 loss 0.036632 acc 0.984127\n",
      "Epoch 22 batch 0 loss 0.036000 acc 0.984375\n",
      "Epoch 22 batch 600 loss 0.036579 acc 0.984127\n",
      "Epoch 23 batch 0 loss 0.036005 acc 0.984375\n",
      "Epoch 23 batch 600 loss 0.036572 acc 0.984127\n",
      "Epoch 24 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 24 batch 600 loss 0.037257 acc 0.984127\n",
      "Epoch 25 batch 0 loss 0.036031 acc 0.984375\n",
      "Epoch 25 batch 600 loss 0.036855 acc 0.984127\n",
      "Epoch 26 batch 0 loss 0.036029 acc 0.984375\n",
      "Epoch 26 batch 600 loss 0.036627 acc 0.984127\n",
      "Epoch 27 batch 0 loss 0.036014 acc 0.984375\n",
      "Epoch 27 batch 600 loss 0.036609 acc 0.984127\n",
      "Epoch 28 batch 0 loss 0.035986 acc 0.984375\n",
      "Epoch 28 batch 600 loss 0.036890 acc 0.984127\n",
      "Epoch 29 batch 0 loss 0.035987 acc 0.984375\n",
      "Epoch 29 batch 600 loss 0.036592 acc 0.984127\n",
      "Validation accuracy: 0.970540\n",
      "Epoch 30 batch 0 loss 0.035999 acc 0.984375\n",
      "Epoch 30 batch 600 loss 0.036557 acc 0.984127\n",
      "Epoch 31 batch 0 loss 0.036005 acc 0.984375\n",
      "Epoch 31 batch 600 loss 0.036566 acc 0.984127\n",
      "Epoch 32 batch 0 loss 0.035995 acc 0.984375\n",
      "Epoch 32 batch 600 loss 0.036667 acc 0.984127\n",
      "Epoch 33 batch 0 loss 0.035986 acc 0.984375\n",
      "Epoch 33 batch 600 loss 0.036620 acc 0.984127\n",
      "Epoch 34 batch 0 loss 0.036031 acc 0.984375\n",
      "Epoch 34 batch 600 loss 0.036622 acc 0.984127\n",
      "Epoch 35 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 35 batch 600 loss 0.036553 acc 0.984127\n",
      "Epoch 36 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 36 batch 600 loss 0.036562 acc 0.984127\n",
      "Epoch 37 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 37 batch 600 loss 0.036553 acc 0.984127\n",
      "Epoch 38 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 38 batch 600 loss 0.036951 acc 0.984127\n",
      "Epoch 39 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 39 batch 600 loss 0.036565 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 40 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 40 batch 600 loss 0.036576 acc 0.984127\n",
      "Epoch 41 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 41 batch 600 loss 0.036552 acc 0.984127\n",
      "Epoch 42 batch 0 loss 0.036000 acc 0.984375\n",
      "Epoch 42 batch 600 loss 0.036564 acc 0.984127\n",
      "Epoch 43 batch 0 loss 0.036000 acc 0.984375\n",
      "Epoch 43 batch 600 loss 0.036559 acc 0.984127\n",
      "Epoch 44 batch 0 loss 0.035990 acc 0.984375\n",
      "Epoch 44 batch 600 loss 0.036555 acc 0.984127\n",
      "Epoch 45 batch 0 loss 0.035991 acc 0.984375\n",
      "Epoch 45 batch 600 loss 0.036597 acc 0.984127\n",
      "Epoch 46 batch 0 loss 0.036003 acc 0.984375\n",
      "Epoch 46 batch 600 loss 0.036575 acc 0.984127\n",
      "Epoch 47 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 47 batch 600 loss 0.036552 acc 0.984127\n",
      "Epoch 48 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 48 batch 600 loss 0.036553 acc 0.984127\n",
      "Epoch 49 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 49 batch 600 loss 0.036553 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 50 batch 0 loss 0.035986 acc 0.984375\n",
      "Epoch 50 batch 600 loss 0.036552 acc 0.984127\n",
      "Epoch 51 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 51 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 52 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 52 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 53 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 53 batch 600 loss 0.036553 acc 0.984127\n",
      "Epoch 54 batch 0 loss 0.035984 acc 0.984375\n",
      "Epoch 54 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 55 batch 0 loss 0.035981 acc 0.984375\n",
      "Epoch 55 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 56 batch 0 loss 0.035993 acc 0.984375\n",
      "Epoch 56 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 57 batch 0 loss 0.036012 acc 0.984375\n",
      "Epoch 57 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 58 batch 0 loss 0.035996 acc 0.984375\n",
      "Epoch 58 batch 600 loss 0.036552 acc 0.984127\n",
      "Epoch 59 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 59 batch 600 loss 0.036552 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 60 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 60 batch 600 loss 0.036826 acc 0.984127\n",
      "Epoch 61 batch 0 loss 0.036220 acc 0.984375\n",
      "Epoch 61 batch 600 loss 0.036554 acc 0.984127\n",
      "Epoch 62 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 62 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 63 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 63 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 64 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 64 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 65 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 65 batch 600 loss 0.036551 acc 0.984127\n",
      "Epoch 66 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 66 batch 600 loss 0.037746 acc 0.984127\n",
      "Epoch 67 batch 0 loss 0.036005 acc 0.984375\n",
      "Epoch 67 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 68 batch 0 loss 0.035981 acc 0.984375\n",
      "Epoch 68 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 69 batch 0 loss 0.035990 acc 0.984375\n",
      "Epoch 69 batch 600 loss 0.036550 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 70 batch 0 loss 0.036000 acc 0.984375\n",
      "Epoch 70 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 71 batch 0 loss 0.036000 acc 0.984375\n",
      "Epoch 71 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 72 batch 0 loss 0.035992 acc 0.984375\n",
      "Epoch 72 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 73 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 73 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 74 batch 0 loss 0.035984 acc 0.984375\n",
      "Epoch 74 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 75 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 75 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 76 batch 0 loss 0.035994 acc 0.984375\n",
      "Epoch 76 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 77 batch 0 loss 0.035996 acc 0.984375\n",
      "Epoch 77 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 78 batch 0 loss 0.035999 acc 0.984375\n",
      "Epoch 78 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 79 batch 0 loss 0.035996 acc 0.984375\n",
      "Epoch 79 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 80 batch 0 loss 0.035989 acc 0.984375\n",
      "Epoch 80 batch 600 loss 0.036813 acc 0.984127\n",
      "Epoch 81 batch 0 loss 0.036242 acc 0.984375\n",
      "Epoch 81 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 82 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 82 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 83 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 83 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 84 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 84 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 85 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 85 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 86 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 86 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 87 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 87 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 88 batch 0 loss 0.035981 acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 89 batch 0 loss 0.035983 acc 0.984375\n",
      "Epoch 89 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 90 batch 0 loss 0.035984 acc 0.984375\n",
      "Epoch 90 batch 600 loss 0.036557 acc 0.984127\n",
      "Epoch 91 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 91 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 92 batch 0 loss 0.036000 acc 0.984375\n",
      "Epoch 92 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 93 batch 0 loss 0.035992 acc 0.984375\n",
      "Epoch 93 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 94 batch 0 loss 0.035991 acc 0.984375\n",
      "Epoch 94 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 95 batch 0 loss 0.035991 acc 0.984375\n",
      "Epoch 95 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 96 batch 0 loss 0.035991 acc 0.984375\n",
      "Epoch 96 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 97 batch 0 loss 0.035989 acc 0.984375\n",
      "Epoch 97 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 98 batch 0 loss 0.035984 acc 0.984375\n",
      "Epoch 98 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 99 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 99 batch 600 loss 0.036550 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 100 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 100 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 101 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 101 batch 600 loss 0.036575 acc 0.984127\n",
      "Epoch 102 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 102 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 103 batch 0 loss 0.035993 acc 0.984375\n",
      "Epoch 103 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 104 batch 0 loss 0.036005 acc 0.984375\n",
      "Epoch 104 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 105 batch 0 loss 0.036004 acc 0.984375\n",
      "Epoch 105 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 106 batch 0 loss 0.036002 acc 0.984375\n",
      "Epoch 106 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 107 batch 0 loss 0.035999 acc 0.984375\n",
      "Epoch 107 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 108 batch 0 loss 0.035997 acc 0.984375\n",
      "Epoch 108 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 109 batch 0 loss 0.035995 acc 0.984375\n",
      "Epoch 109 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 110 batch 0 loss 0.035991 acc 0.984375\n",
      "Epoch 110 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 111 batch 0 loss 0.035987 acc 0.984375\n",
      "Epoch 111 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 112 batch 0 loss 0.035983 acc 0.984375\n",
      "Epoch 112 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 113 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 113 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 114 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 114 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 115 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 115 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 116 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 116 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 117 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 117 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 118 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 118 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 119 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 119 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 120 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 120 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 121 batch 0 loss 0.035981 acc 0.984375\n",
      "Epoch 121 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 122 batch 0 loss 0.035981 acc 0.984375\n",
      "Epoch 122 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 123 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 123 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 124 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 124 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 125 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 125 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 126 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 126 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 127 batch 0 loss 0.035984 acc 0.984375\n",
      "Epoch 127 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 128 batch 0 loss 0.035983 acc 0.984375\n",
      "Epoch 128 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 129 batch 0 loss 0.035983 acc 0.984375\n",
      "Epoch 129 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 130 batch 0 loss 0.035982 acc 0.984375\n",
      "Epoch 130 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 131 batch 0 loss 0.035983 acc 0.984375\n",
      "Epoch 131 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 132 batch 0 loss 0.035984 acc 0.984375\n",
      "Epoch 132 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 133 batch 0 loss 0.035983 acc 0.984375\n",
      "Epoch 133 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 134 batch 0 loss 0.035980 acc 0.984375\n",
      "Epoch 134 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 135 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 135 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 136 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 136 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 137 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 137 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 138 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 138 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 139 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 139 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 140 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 140 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 141 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 141 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 142 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 142 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 143 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 143 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 144 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 144 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 145 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 145 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 146 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 146 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 147 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 147 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 148 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 148 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 149 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 149 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 150 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 150 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 151 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 151 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 152 batch 0 loss 0.035985 acc 0.984375\n",
      "Epoch 152 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 153 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 153 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 154 batch 0 loss 0.035979 acc 0.984375\n",
      "Epoch 154 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 155 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 155 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 156 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 156 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 157 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 157 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 158 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 158 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 159 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 159 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 160 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 160 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 161 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 161 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 162 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 162 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 163 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 163 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 164 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 164 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 165 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 165 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 166 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 166 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 167 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 167 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 168 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 168 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 169 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 169 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 170 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 170 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 171 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 171 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 172 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 172 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 173 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 173 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 174 batch 0 loss 0.035978 acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 175 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 175 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 176 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 176 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 177 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 177 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 178 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 178 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 179 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 179 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 180 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 180 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 181 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 181 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 182 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 182 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 183 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 183 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 184 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 184 batch 600 loss 0.036550 acc 0.984127\n",
      "Epoch 185 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 185 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 186 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 186 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 187 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 187 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 188 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 188 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 189 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 189 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n",
      "Epoch 190 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 190 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 191 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 191 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 192 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 192 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 193 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 193 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 194 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 194 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 195 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 195 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 196 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 196 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 197 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 197 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 198 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 198 batch 600 loss 0.036549 acc 0.984127\n",
      "Epoch 199 batch 0 loss 0.035978 acc 0.984375\n",
      "Epoch 199 batch 600 loss 0.036549 acc 0.984127\n",
      "Validation accuracy: 0.970846\n"
     ]
    }
   ],
   "source": [
    "protoNNTrainer.train(BATCH_SIZE, NUM_EPOCHS, x_train, x_test, y_train, y_test, printStep=600, valStep=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-30T09:09:00.026Z"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T10:25:13.469355Z",
     "start_time": "2019-06-30T10:25:13.447362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy tensor(0.9708, dtype=torch.float64)\n",
      "Model size constraint (Bytes):  6560\n",
      "Number of non-zeros:  1640\n",
      "Actual model size:  6560\n",
      "Actual non-zeros:  1640\n"
     ]
    }
   ],
   "source": [
    " x_, y_= torch.Tensor(x_test), torch.Tensor(y_test)\n",
    "logits = protoNNObj.forward(x_)\n",
    "_, predictions = torch.max(logits, dim=1)\n",
    "_, target = torch.max(y_, dim=1)\n",
    "acc, count = protoNNTrainer.accuracy(predictions, target)\n",
    "W, B, Z, gamma  = protoNNObj.getModelMatrices()\n",
    "matrixList = [W, B, Z]\n",
    "matrixList = [x.detach().numpy() for x in matrixList]\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)\n",
    "nnz, size, sparse = helper.getModelSize(matrixList, sparcityList,\n",
    "                                        expected=False)\n",
    "print(\"Actual model size: \", size)\n",
    "print(\"Actual non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 124)\n",
      "(20, 10)\n",
      "(20, 10)\n"
     ]
    }
   ],
   "source": [
    "W = W.detach().numpy()\n",
    "B = B.detach().numpy()\n",
    "Z = Z.detach().numpy()\n",
    "W = np.transpose(W)\n",
    "B = np.transpose(B)\n",
    "Z = np.transpose(Z)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "print(Z.shape)\n",
    "np.savetxt(\"W\", W, fmt=\"%f\", delimiter=\",\")\n",
    "np.savetxt(\"B\", B, fmt=\"%f\", delimiter=\",\")\n",
    "np.savetxt(\"Z\", Z, fmt=\"%f\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
